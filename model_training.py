# -*- coding: utf-8 -*-
"""Model Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14OX8Wd6O8a-C7S8n6mGhNHuD54RJzIsd

# Prerequisites
"""

import numpy as np
import pandas as pd
from tensorflow import keras
from keras.models import Sequential, model_from_json
from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout

"""# Preprocessing

## Read data file and convert to dataframe:
"""

data = pd.read_csv('train.csv')
df_data = pd.DataFrame(data.values, columns=['Emotion', 'Pixels'])
df_data

"""## Convert 'Emotion' column to array:"""

labels = np.array(df_data['Emotion'], dtype=np.float32)

"""## Convert 'Pixels' column to array:"""

# Convert column to list
pixels = list(df_data['Pixels'])

for i in range(len(pixels)):
    # Split single string into many strings
    pixels[i] = pixels[i].split()
    
    for j in range(len(pixels[i])):
        # Convert strings to float objects
        pixels[i][j] = float(pixels[i][j])


        
for i in range(1, len(pixels)):
    # Create list of all pixel values
    ls_pixels = pixels[0]
    ls_pixels.extend(pixels[i])
    
# Reshape and normalise pixel list
pixels = np.array(ls_pixels,
                  dtype=np.float32).reshape(len(df_data.index),
                                            48,48,1) / 255.0

"""# Model

## Create the model:
"""

model = Sequential()

# First convolutional layer
model.add(Conv2D(input_shape=(48,48,1),
                 filters=64, kernel_size=2, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

#Second convolutional layer
model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

# Third convolutional layer
model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

# Flatten from 4D to 2D
model.add(Flatten())

# Dense layer
model.add(Dense(units=100, activation='relu'))

#Apply 20% dropout rate
model.add(Dropout(rate=0.2))

# Dense output layer
model.add(Dense(units=7, activation='softmax'))

model.summary()

"""## Compile the model:"""

optimizer = 'adam' 
loss = 'sparse_categorical_crossentropy' 
metrics = ['accuracy']

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

"""## Train the model:"""

# Analyse data in batches of this size
batch_size = 20

# Run model for this many epochs
epochs = 60

model.fit(pixels, labels, epochs=epochs,
          batch_size=batch_size, validation_split=0.2)

"""## Save the model:"""

# Serialise model to JSON
model_json = model.to_json()
with open('model.json', 'w') as json_file:
    json_file.write(model_json)

# Serialise weights to HDF5
model.save_weights('model.h5')
print('Saved trained model to disk')